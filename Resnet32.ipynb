{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.utils.data as data\n",
    "\n",
    "from model import ResidualBlock, ResNet\n",
    "from train_eval_util import train, evaluate, calculate_accuracy, epoch_time\n",
    "from getCIFAR10 import train_data, valid_data, test_data\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')  \n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f\"Selected device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 500\n",
    "\n",
    "train_iterator = DataLoader(train_data, batch_size= BATCH_SIZE, shuffle=True)\n",
    "\n",
    "valid_iterator =  DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "test_iterator =  DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of layers: 32\n",
      "Total number of parameters: 466906\n"
     ]
    }
   ],
   "source": [
    "model = ResNet(ResidualBlock, [5, 5, 5]).to(device)\n",
    "\n",
    "\n",
    "total_layers = sum([1 for _ in model.modules() \n",
    "    if isinstance(_, nn.Conv2d) or isinstance(_, nn.Linear)]) - 2 # subtract input and output layers\n",
    "    \n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total number of layers: {total_layers}\")\n",
    "print(f\"Total number of parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 1.788 | Train Acc: 32.28%\n",
      "\t Val. Loss: 1.983 |  Val. Acc: 32.46%\n",
      "Epoch: 02 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 1.372 | Train Acc: 49.30%\n",
      "\t Val. Loss: 1.521 |  Val. Acc: 48.74%\n",
      "Epoch: 03 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 1.121 | Train Acc: 59.64%\n",
      "\t Val. Loss: 1.044 |  Val. Acc: 62.36%\n",
      "Epoch: 04 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.930 | Train Acc: 66.69%\n",
      "\t Val. Loss: 1.044 |  Val. Acc: 62.64%\n",
      "Epoch: 05 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.807 | Train Acc: 71.69%\n",
      "\t Val. Loss: 0.798 |  Val. Acc: 72.98%\n",
      "Epoch: 06 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.652 | Train Acc: 77.09%\n",
      "\t Val. Loss: 0.621 |  Val. Acc: 78.56%\n",
      "Epoch: 07 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.609 | Train Acc: 78.67%\n",
      "\t Val. Loss: 0.605 |  Val. Acc: 79.06%\n",
      "Epoch: 08 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.589 | Train Acc: 79.41%\n",
      "\t Val. Loss: 0.586 |  Val. Acc: 79.14%\n",
      "Epoch: 09 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.574 | Train Acc: 79.90%\n",
      "\t Val. Loss: 0.583 |  Val. Acc: 80.10%\n",
      "Epoch: 10 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.560 | Train Acc: 80.59%\n",
      "\t Val. Loss: 0.574 |  Val. Acc: 80.24%\n",
      "Epoch: 11 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.541 | Train Acc: 81.21%\n",
      "\t Val. Loss: 0.551 |  Val. Acc: 81.28%\n",
      "Epoch: 12 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.532 | Train Acc: 81.51%\n",
      "\t Val. Loss: 0.549 |  Val. Acc: 81.22%\n",
      "Epoch: 13 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.530 | Train Acc: 81.56%\n",
      "\t Val. Loss: 0.548 |  Val. Acc: 81.20%\n",
      "Epoch: 14 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.530 | Train Acc: 81.47%\n",
      "\t Val. Loss: 0.547 |  Val. Acc: 81.22%\n",
      "Epoch: 15 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.526 | Train Acc: 81.67%\n",
      "\t Val. Loss: 0.546 |  Val. Acc: 81.34%\n",
      "Epoch: 16 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.524 | Train Acc: 81.58%\n",
      "\t Val. Loss: 0.546 |  Val. Acc: 81.30%\n",
      "Epoch: 17 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.527 | Train Acc: 81.49%\n",
      "\t Val. Loss: 0.546 |  Val. Acc: 81.24%\n",
      "Epoch: 18 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.523 | Train Acc: 81.65%\n",
      "\t Val. Loss: 0.546 |  Val. Acc: 81.20%\n",
      "Epoch: 19 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.526 | Train Acc: 81.90%\n",
      "\t Val. Loss: 0.545 |  Val. Acc: 81.28%\n",
      "Epoch: 20 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.524 | Train Acc: 81.69%\n",
      "\t Val. Loss: 0.546 |  Val. Acc: 81.32%\n",
      "Epoch: 21 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 0.525 | Train Acc: 81.68%\n",
      "\t Val. Loss: 0.545 |  Val. Acc: 81.16%\n",
      "Epoch: 22 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.524 | Train Acc: 81.96%\n",
      "\t Val. Loss: 0.546 |  Val. Acc: 81.18%\n",
      "Epoch: 23 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.525 | Train Acc: 81.67%\n",
      "\t Val. Loss: 0.545 |  Val. Acc: 81.34%\n",
      "Epoch: 24 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.524 | Train Acc: 81.62%\n",
      "\t Val. Loss: 0.545 |  Val. Acc: 81.20%\n",
      "Epoch: 25 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.523 | Train Acc: 81.88%\n",
      "\t Val. Loss: 0.546 |  Val. Acc: 81.12%\n",
      "Epoch: 26 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.523 | Train Acc: 81.76%\n",
      "\t Val. Loss: 0.546 |  Val. Acc: 81.26%\n",
      "Epoch: 27 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.522 | Train Acc: 81.78%\n",
      "\t Val. Loss: 0.546 |  Val. Acc: 81.36%\n",
      "Epoch: 28 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.525 | Train Acc: 81.81%\n",
      "\t Val. Loss: 0.547 |  Val. Acc: 81.14%\n",
      "Epoch: 29 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.522 | Train Acc: 81.79%\n",
      "\t Val. Loss: 0.545 |  Val. Acc: 81.26%\n",
      "Epoch: 30 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.523 | Train Acc: 81.65%\n",
      "\t Val. Loss: 0.546 |  Val. Acc: 81.12%\n",
      "Epoch: 31 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.523 | Train Acc: 81.87%\n",
      "\t Val. Loss: 0.545 |  Val. Acc: 81.28%\n",
      "Epoch: 32 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.524 | Train Acc: 81.77%\n",
      "\t Val. Loss: 0.545 |  Val. Acc: 81.24%\n",
      "Epoch: 33 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.523 | Train Acc: 81.75%\n",
      "\t Val. Loss: 0.545 |  Val. Acc: 81.24%\n",
      "Epoch: 34 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.524 | Train Acc: 81.82%\n",
      "\t Val. Loss: 0.545 |  Val. Acc: 81.28%\n",
      "Epoch: 35 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.524 | Train Acc: 81.81%\n",
      "\t Val. Loss: 0.545 |  Val. Acc: 81.22%\n",
      "Epoch: 36 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.522 | Train Acc: 81.80%\n",
      "\t Val. Loss: 0.545 |  Val. Acc: 81.22%\n",
      "Epoch: 37 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.522 | Train Acc: 81.73%\n",
      "\t Val. Loss: 0.545 |  Val. Acc: 81.24%\n",
      "Epoch: 38 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.523 | Train Acc: 81.69%\n",
      "\t Val. Loss: 0.545 |  Val. Acc: 81.26%\n",
      "Epoch: 39 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.525 | Train Acc: 81.57%\n",
      "\t Val. Loss: 0.546 |  Val. Acc: 81.04%\n",
      "Epoch: 40 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.523 | Train Acc: 81.80%\n",
      "\t Val. Loss: 0.545 |  Val. Acc: 81.32%\n",
      "Epoch: 41 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.522 | Train Acc: 81.78%\n",
      "\t Val. Loss: 0.545 |  Val. Acc: 81.26%\n",
      "Epoch: 42 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.522 | Train Acc: 81.82%\n",
      "\t Val. Loss: 0.547 |  Val. Acc: 81.14%\n",
      "Epoch: 43 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.521 | Train Acc: 81.76%\n",
      "\t Val. Loss: 0.546 |  Val. Acc: 81.24%\n",
      "Epoch: 44 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.520 | Train Acc: 81.84%\n",
      "\t Val. Loss: 0.545 |  Val. Acc: 81.30%\n",
      "Epoch: 45 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.522 | Train Acc: 81.80%\n",
      "\t Val. Loss: 0.546 |  Val. Acc: 81.22%\n",
      "Epoch: 46 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.528 | Train Acc: 81.74%\n",
      "\t Val. Loss: 0.545 |  Val. Acc: 81.34%\n",
      "Epoch: 47 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.526 | Train Acc: 81.61%\n",
      "\t Val. Loss: 0.546 |  Val. Acc: 81.24%\n",
      "Epoch: 48 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.526 | Train Acc: 81.68%\n",
      "\t Val. Loss: 0.545 |  Val. Acc: 81.28%\n",
      "Epoch: 49 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.525 | Train Acc: 81.62%\n",
      "\t Val. Loss: 0.545 |  Val. Acc: 81.22%\n",
      "Epoch: 50 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.527 | Train Acc: 81.49%\n",
      "\t Val. Loss: 0.545 |  Val. Acc: 81.24%\n",
      "Epoch: 51 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.522 | Train Acc: 81.93%\n",
      "\t Val. Loss: 0.545 |  Val. Acc: 81.26%\n",
      "Epoch: 52 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.526 | Train Acc: 81.60%\n",
      "\t Val. Loss: 0.545 |  Val. Acc: 81.22%\n",
      "Epoch: 53 | Epoch Time: 0m 15s\n",
      "\tTrain Loss: 0.525 | Train Acc: 81.61%\n",
      "\t Val. Loss: 0.546 |  Val. Acc: 81.36%\n",
      "Epoch: 54 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.521 | Train Acc: 81.81%\n",
      "\t Val. Loss: 0.545 |  Val. Acc: 81.24%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/users/Vivaldi/Documents/Github/ECE7123_miniproject/Resnet32.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/c/users/Vivaldi/Documents/Github/ECE7123_miniproject/Resnet32.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/c/users/Vivaldi/Documents/Github/ECE7123_miniproject/Resnet32.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/c/users/Vivaldi/Documents/Github/ECE7123_miniproject/Resnet32.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(model, train_iterator, optimizer, criterion, device)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/c/users/Vivaldi/Documents/Github/ECE7123_miniproject/Resnet32.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m     scheduler\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/mnt/c/users/Vivaldi/Documents/Github/ECE7123_miniproject/Resnet32.ipynb#X20sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     valid_loss, valid_acc \u001b[39m=\u001b[39m evaluate(model, valid_iterator, criterion, device)\n",
      "File \u001b[0;32m/mnt/c/users/Vivaldi/Documents/Github/ECE7123_miniproject/train_eval_util.py:23\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m     19\u001b[0m epoch_acc \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     21\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> 23\u001b[0m \u001b[39mfor\u001b[39;00m (x, y) \u001b[39min\u001b[39;00m iterator:\n\u001b[1;32m     25\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     26\u001b[0m     y \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchomp/lib/python3.8/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/anaconda3/envs/torchomp/lib/python3.8/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchomp/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/torchomp/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/anaconda3/envs/torchomp/lib/python3.8/site-packages/torch/utils/data/dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[0;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[0;32m~/anaconda3/envs/torchomp/lib/python3.8/site-packages/torchvision/datasets/cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img)\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[0;32m~/anaconda3/envs/torchomp/lib/python3.8/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/anaconda3/envs/torchomp/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/torchomp/lib/python3.8/site-packages/torchvision/transforms/transforms.py:719\u001b[0m, in \u001b[0;36mRandomHorizontalFlip.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m    712\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    713\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be flipped.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    717\u001b[0m \u001b[39m        PIL Image or Tensor: Randomly flipped image.\u001b[39;00m\n\u001b[1;32m    718\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 719\u001b[0m     \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mrand(\u001b[39m1\u001b[39m) \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mp:\n\u001b[1;32m    720\u001b[0m         \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mhflip(img)\n\u001b[1;32m    721\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "train_acc_history = []\n",
    "train_loss_history = []\n",
    "valid_acc_history = []\n",
    "valid_loss_history = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion, device)\n",
    "    scheduler.step()\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, device)\n",
    "        \n",
    "    end_time = time.time()\n",
    "\n",
    "        \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "    train_acc_history.append(train_acc)\n",
    "    train_loss_history.append( train_loss)\n",
    "    valid_acc_history.append(valid_acc)\n",
    "    valid_loss_history.append(valid_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig,(ax1, ax2) = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "ax1.plot(range(num_epochs), train_loss_history, '--r')\n",
    "ax1.plot(range(num_epochs), valid_loss_history, '-g')\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend(['train', 'valid'])\n",
    "\n",
    "ax2.plot(range(num_epochs), train_acc_history, '--r')\n",
    "ax2.plot(range(num_epochs), valid_acc_history, '-g')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_xlabel('Accuracy')\n",
    "ax1.legend(['train', 'valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'resnet32.pt')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "hw3prob5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
